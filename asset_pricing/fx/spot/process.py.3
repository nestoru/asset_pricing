# asset_pricing/fx/spot/process.py

import sys
import os
import logging
from typing import Union
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

from asset_pricing.fx.spot.config import (
    load_config,
    MockedFXSpotProfile,
    AlphaVantageFXSpotProfile,
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_spark_session(spark_config) -> SparkSession:
    builder = SparkSession.builder.appName(spark_config.app_name)

    # Set master if specified
    if spark_config.master:
        builder = builder.master(spark_config.master)

    # Set other Spark configurations
    config_dict = spark_config.config
    for key, value in config_dict.items():
        builder = builder.config(key, value)

    spark = builder.getOrCreate()
    return spark

def process_fx_spot_rates(config_file: str, profile_name: str) -> None:
    config = load_config(config_file)
    try:
        profile = config.get_spot_profile(profile_name)
    except ValueError as e:
        logger.error(str(e))
        sys.exit(1)

    spark = get_spark_session(config.spark)

    try:
        if isinstance(profile, (MockedFXSpotProfile, AlphaVantageFXSpotProfile)):
            process_fx_data(spark, profile, config.database)
        else:
            logger.error(f"Unsupported profile type for '{profile_name}'")
            sys.exit(1)
    finally:
        spark.stop()

def process_fx_data(spark, profile, db_config) -> None:
    # Set up database connection properties
    jdbc_url = f"jdbc:postgresql://{db_config.host}:{db_config.port}/{db_config.database}"
    connection_properties = {
        "user": db_config.user,
        "password": db_config.password,
        "driver": "org.postgresql.Driver"
    }

    files_to_process = []

    # Determine data source and get files to process
    if profile.local_caching_path:
        data_path = profile.local_caching_path
        if not os.path.exists(data_path):
            logger.error(f"Local cache path does not exist: {data_path}")
            return

        # List all JSON files
        for file in os.listdir(data_path):
            if file.endswith('.json'):
                files_to_process.append(os.path.join(data_path, file))
                logger.debug(f"Found file to process: {file}")

    elif profile.remote_caching:
        remote = profile.remote_caching
        # Initialize Azure blob client
        from azure.storage.blob import BlobServiceClient
        blob_service = BlobServiceClient(
            account_url=f"https://{remote.storage_account_name}.blob.core.windows.net",
            credential=remote.storage_account_key
        )
        container_client = blob_service.get_container_client(remote.container_name)
        
        # List all blobs in the container with the specified prefix
        blob_prefix = remote.blob_path.rstrip('/') + '/'
        blobs = container_client.list_blobs(name_starts_with=blob_prefix)
        
        for blob in blobs:
            if blob.name.endswith('.json'):
                files_to_process.append(
                    f"wasbs://{remote.container_name}@{remote.storage_account_name}.blob.core.windows.net/{blob.name}"
                )
                logger.debug(f"Found blob to process: {blob.name}")
        
        # Set up Spark for Azure access
        spark.conf.set(
            f"fs.azure.account.key.{remote.storage_account_name}.blob.core.windows.net",
            remote.storage_account_key
        )
    else:
        logger.error("No caching configuration found.")
        return

    if not files_to_process:
        logger.info("No files found to process")
        return

    try:
        # Read the new files
        logger.info(f"Reading {len(files_to_process)} files")
        new_data_df = spark.read.json(files_to_process)

        # Check if DataFrame is empty
        if new_data_df.rdd.isEmpty():
            logger.info("No data found in files")
            return

        # Cast columns to appropriate types
        new_data_df = new_data_df.withColumn("date", col("date").cast("timestamp"))
        new_data_df = new_data_df.withColumn("rate", col("rate").cast("double"))

        # Remove duplicates within the new data
        new_data_df = new_data_df.dropDuplicates(["symbol", "date"])

        # Read existing data from database
        logger.info("Reading existing data from database")
        existing_data_df = spark.read.jdbc(
            url=jdbc_url,
            table=f"{db_config.db_schema}.fx_spot_rates",
            properties=connection_properties
        )

        # Anti-join to get only new records
        df_to_insert = new_data_df.join(
            existing_data_df,
            (new_data_df.symbol == existing_data_df.symbol) & 
            (new_data_df.date == existing_data_df.date),
            "left_anti"
        )

        # If nothing new to insert, return
        if df_to_insert.rdd.isEmpty():
            logger.info("No new unique records to insert")
            return

        record_count = df_to_insert.count()
        logger.info(f"Found {record_count} new unique records to insert")

        # Write new records to PostgreSQL
        df_to_insert.write.jdbc(
            url=jdbc_url,
            table=f"{db_config.db_schema}.fx_spot_rates",
            mode="append",
            properties=connection_properties
        )
        logger.info(f"Successfully inserted {record_count} new records")

    except Exception as e:
        logger.error(f"Error processing data: {str(e)}")
        raise

def main() -> None:
    if len(sys.argv) != 3:
        print("Usage: process-fx-spot <config_file> <profile_name>")
        sys.exit(1)
    
    config_file = sys.argv[1]
    profile_name = sys.argv[2]
    
    try:
        process_fx_spot_rates(config_file, profile_name)
    except Exception as e:
        logger.error(f"Process failed: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
